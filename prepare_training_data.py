# coding=utf-8
"""
æ–¹è¨€è®­ç»ƒæ•°æ®å‡†å¤‡è„šæœ¬

åŠŸèƒ½ï¼š
1. ç”Ÿæˆ instruct æ–‡ä»¶ï¼ˆCosyVoice3 éœ€è¦ï¼Œä½¿ç”¨ä¸­æ–‡æŒ‡ä»¤ï¼‰
2. å°† MP3 è½¬æ¢ä¸º WAVï¼ˆ16kHz monoï¼‰ã€å¯é€‰ã€‘
3. åˆå¹¶æ‰€æœ‰æ–¹è¨€æ•°æ®åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ç»Ÿä¸€è®­ç»ƒ
4. éªŒè¯æ•°æ®å®Œæ•´æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
    python prepare_training_data.py --mode all        # å®Œæ•´å‡†å¤‡
    python prepare_training_data.py --mode instruct   # åªç”Ÿæˆ instruct
    python prepare_training_data.py --mode combine    # åªåˆå¹¶æ•°æ®
    python prepare_training_data.py --mode validate   # éªŒè¯æ•°æ®

ä½œè€…: Antigravity AI Assistant
æ—¥æœŸ: 2026-01-19
"""

import os
import sys
import argparse
import subprocess
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# é…ç½®
DATASET_DIR = "dataset"
COMBINED_DIR = "combined"  # åˆå¹¶åçš„ç›®å½•å
SAMPLE_RATE = 16000

# æ–¹è¨€æŒ‡ä»¤é…ç½®ï¼ˆä¸­æ–‡ï¼‰
INSTRUCT_TEMPLATES = {
    # åŒ—æ–¹æ–¹è¨€
    "dongbei": "è¯·ç”¨ä¸œåŒ—è¯è¯´ã€‚<|endofprompt|>",
    "tianjin": "è¯·ç”¨å¤©æ´¥è¯è¯´ã€‚<|endofprompt|>",
    "xian": "è¯·ç”¨è¥¿å®‰è¯è¯´ã€‚<|endofprompt|>",
    "henan": "è¯·ç”¨æ²³å—è¯è¯´ã€‚<|endofprompt|>",
    "zhengzhou": "è¯·ç”¨éƒ‘å·è¯è¯´ã€‚<|endofprompt|>",
    
    # è¥¿å—å®˜è¯
    "sichuan": "è¯·ç”¨å››å·è¯è¯´ã€‚<|endofprompt|>",
    "chuanyu": "è¯·ç”¨å·æ¸æ–¹è¨€è¯´ã€‚<|endofprompt|>",
    "chongqing": "è¯·ç”¨é‡åº†è¯è¯´ã€‚<|endofprompt|>",
    
    # æ¹˜è¯­
    "hunan": "è¯·ç”¨æ¹–å—è¯è¯´ã€‚<|endofprompt|>",
    "changsha": "è¯·ç”¨é•¿æ²™è¯è¯´ã€‚<|endofprompt|>",
    "hunan_pu": "è¯·ç”¨æ¹–å—æ™®é€šè¯è¯´ã€‚<|endofprompt|>",
    
    # ç²¤è¯­
    "cantonese": "è¯·ç”¨ç²¤è¯­è¯´ã€‚<|endofprompt|>",
    "yueyu": "è¯·ç”¨ç²¤è¯­è¯´ã€‚<|endofprompt|>",
    "guangxi": "è¯·ç”¨å¹¿è¥¿è¯è¯´ã€‚<|endofprompt|>",
    
    # å´è¯­
    "shanghai": "è¯·ç”¨ä¸Šæµ·è¯è¯´ã€‚<|endofprompt|>",
    
    # æƒ…æ„Ÿï¼ˆemotion ä½œä¸ºç‰¹æ®Šæ–¹è¨€å¤„ç†ï¼‰
    "emotion": "è¯·ç”¨æ™®é€šè¯è¯´ã€‚<|endofprompt|>",  # æƒ…æ„Ÿæ•°æ®å·²æœ‰é€å¥instructï¼Œæ­¤ä¸ºfallback
    
    # é»˜è®¤
    "default": "è¯·ç”¨æ–¹è¨€è¯´ã€‚<|endofprompt|>"
}

# æ–¹è¨€ä¸­æ–‡åç§°æ˜ å°„
DIALECT_NAMES = {
    "dongbei": "ä¸œåŒ—è¯",
    "tianjin": "å¤©æ´¥è¯",
    "xian": "è¥¿å®‰è¯",
    "henan": "æ²³å—è¯",
    "zhengzhou": "éƒ‘å·è¯",
    "sichuan": "å››å·è¯",
    "chuanyu": "å·æ¸æ–¹è¨€",
    "chongqing": "é‡åº†è¯",
    "hunan": "æ¹–å—è¯",
    "changsha": "é•¿æ²™è¯",
    "hunan_pu": "æ¹–å—æ™®é€šè¯",
    "cantonese": "ç²¤è¯­",
    "yueyu": "ç²¤è¯­",
    "guangxi": "å¹¿è¥¿è¯",
    "shanghai": "ä¸Šæµ·è¯",
    "emotion": "æƒ…æ„Ÿæ•°æ®",
}


def check_ffmpeg() -> bool:
    """æ£€æŸ¥ ffmpeg æ˜¯å¦å¯ç”¨"""
    return shutil.which("ffmpeg") is not None


def convert_single_audio(args: Tuple[str, str]) -> Tuple[bool, str]:
    """è½¬æ¢å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
    src_path, dst_path = args
    
    if os.path.exists(dst_path):
        return True, dst_path
    
    try:
        result = subprocess.run([
            "ffmpeg", "-y", "-i", src_path,
            "-ar", str(SAMPLE_RATE),
            "-ac", "1",
            "-acodec", "pcm_s16le",
            dst_path
        ], capture_output=True, timeout=30)
        
        if result.returncode == 0:
            return True, dst_path
        else:
            return False, f"FFmpeg error: {result.stderr.decode()[:100]}"
    except Exception as e:
        return False, str(e)


def generate_instruct_file(data_dir: Path, dialect: str, force: bool = False) -> int:
    """
    ç”Ÿæˆ instruct æ–‡ä»¶ï¼ˆä¸­æ–‡æŒ‡ä»¤ï¼‰
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        dialect: æ–¹è¨€åç§°
        force: æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²æœ‰çš„ instruct æ–‡ä»¶
    
    Returns:
        ç”Ÿæˆçš„æ¡ç›®æ•°
    """
    text_file = data_dir / "text"
    instruct_file = data_dir / "instruct"
    # ä¹Ÿæ£€æŸ¥ instruct.txtï¼ˆESDæ•°æ®é›†æ ¼å¼ï¼‰
    instruct_txt_file = data_dir / "instruct.txt"
    
    if not text_file.exists():
        print(f"  âŒ é”™è¯¯: {text_file} ä¸å­˜åœ¨")
        return 0
    
    # å¦‚æœå·²å­˜åœ¨ instruct æˆ– instruct.txtï¼Œä¸”ä¸å¼ºåˆ¶è¦†ç›–ï¼Œåˆ™è·³è¿‡
    if not force:
        if instruct_file.exists():
            with open(instruct_file, 'r', encoding='utf-8') as f:
                count = sum(1 for _ in f)
            print(f"  â­ï¸ è·³è¿‡: instruct å·²å­˜åœ¨ ({count} æ¡)")
            return count
        if instruct_txt_file.exists():
            # å°† instruct.txt å¤åˆ¶ä¸º instructï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
            import shutil
            shutil.copy(instruct_txt_file, instruct_file)
            with open(instruct_file, 'r', encoding='utf-8') as f:
                count = sum(1 for _ in f)
            print(f"  âœ… å¤åˆ¶ instruct.txt -> instruct ({count} æ¡)")
            return count
    
    instruct_text = INSTRUCT_TEMPLATES.get(dialect, INSTRUCT_TEMPLATES["default"])
    dialect_name = DIALECT_NAMES.get(dialect, dialect)
    
    count = 0
    with open(text_file, 'r', encoding='utf-8') as f_in, \
         open(instruct_file, 'w', encoding='utf-8') as f_out:
        for line in f_in:
            parts = line.strip().split(maxsplit=1)
            if len(parts) >= 1:
                utt_id = parts[0]
                f_out.write(f"{utt_id} {instruct_text}\n")
                count += 1
    
    print(f"  âœ… ç”Ÿæˆ instruct: {count} æ¡ (æŒ‡ä»¤: {instruct_text[:20]}...)")
    return count


def combine_dialect_data(
    dataset_dir: Path, 
    dialects: List[str], 
    combined_dir: Path,
    extra_dirs: Optional[List[Path]] = None
) -> Dict[str, int]:
    """
    åˆå¹¶æ‰€æœ‰æ–¹è¨€æ•°æ®åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹
    
    Args:
        dataset_dir: ä¸»æ•°æ®é›†ç›®å½•
        dialects: æ–¹è¨€åˆ—è¡¨ï¼ˆç›¸å¯¹äº dataset_dirï¼‰
        combined_dir: è¾“å‡ºçš„åˆå¹¶ç›®å½•
        extra_dirs: é¢å¤–çš„æ•°æ®ç›®å½•åˆ—è¡¨ï¼ˆç»å¯¹è·¯å¾„ï¼Œç›´æ¥åŒ…å« wav.scp ç­‰æ–‡ä»¶ï¼‰
    
    ç”Ÿæˆçš„æ–‡ä»¶:
    - wav.scp: åˆå¹¶çš„éŸ³é¢‘è·¯å¾„ç´¢å¼•
    - text: åˆå¹¶çš„æ–‡æœ¬
    - utt2spk: è¯­éŸ³åˆ°è¯´è¯äººæ˜ å°„
    - spk2utt: è¯´è¯äººåˆ°è¯­éŸ³æ˜ å°„
    - instruct: åˆå¹¶çš„æŒ‡ä»¤
    """
    print(f"\nğŸ“¦ åˆå¹¶æ–¹è¨€æ•°æ®åˆ°: {combined_dir}")
    
    # åˆ›å»ºåˆå¹¶ç›®å½•
    combined_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–åˆå¹¶æ–‡ä»¶
    files_to_merge = ["wav.scp", "text", "utt2spk", "instruct"]
    merged_data = {f: [] for f in files_to_merge}
    spk2utt_data = {}  # éœ€è¦ç‰¹æ®Šå¤„ç†
    
    stats = {
        "total_utts": 0,
        "total_speakers": 0,
        "dialects_processed": 0
    }
    
    for dialect in dialects:
        data_dir = dataset_dir / dialect
        
        # è·³è¿‡ combined ç›®å½•ï¼Œé¿å…å¾ªç¯å¼•ç”¨
        if dialect == COMBINED_DIR or dialect == "combined":
            print(f"  â­ï¸ è·³è¿‡ combined ç›®å½•")
            continue
        
        if not data_dir.exists():
            print(f"  âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„ç›®å½•: {dialect}")
            continue
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
        if not (data_dir / "text").exists():
            print(f"  âš ï¸ è·³è¿‡ {dialect}: ç¼ºå°‘ text æ–‡ä»¶")
            continue
        
        print(f"  ğŸ“‚ å¤„ç† {dialect}...")
        dialect_utt_count = 0
        
        for filename in files_to_merge:
            file_path = data_dir / filename
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            merged_data[filename].append(line)
                            if filename == "text":
                                dialect_utt_count += 1
        
        # å¤„ç† spk2utt
        spk2utt_file = data_dir / "spk2utt"
        if spk2utt_file.exists():
            with open(spk2utt_file, 'r', encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        spk = parts[0]
                        utts = parts[1:]
                        if spk not in spk2utt_data:
                            spk2utt_data[spk] = []
                        spk2utt_data[spk].extend(utts)
        
        stats["total_utts"] += dialect_utt_count
        stats["dialects_processed"] += 1
        print(f"     è¯­éŸ³æ•°: {dialect_utt_count}")
    
    # å†™å…¥åˆå¹¶æ–‡ä»¶
    print("\n  ğŸ“ å†™å…¥åˆå¹¶æ–‡ä»¶...")
    
    for filename, lines in merged_data.items():
        if lines:
            output_file = combined_dir / filename
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines) + '\n')
            print(f"     {filename}: {len(lines)} è¡Œ")
    
    # å†™å…¥ spk2utt
    if spk2utt_data:
        spk2utt_file = combined_dir / "spk2utt"
        with open(spk2utt_file, 'w', encoding='utf-8') as f:
            for spk, utts in spk2utt_data.items():
                f.write(f"{spk} {' '.join(utts)}\n")
        stats["total_speakers"] = len(spk2utt_data)
        print(f"     spk2utt: {len(spk2utt_data)} ä¸ªè¯´è¯äºº")
    
    # å¤„ç†é¢å¤–ç›®å½•
    if extra_dirs:
        print(f"\n  ğŸ“‚ å¤„ç†é¢å¤–æ•°æ®ç›®å½•...")
        for extra_dir in extra_dirs:
            extra_path = Path(extra_dir)
            if not extra_path.exists():
                print(f"  âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„ç›®å½•: {extra_dir}")
                continue
            
            # è·³è¿‡ combined ç›®å½•ï¼Œé¿å…å¾ªç¯å¼•ç”¨
            if extra_path.name == COMBINED_DIR or extra_path.name == "combined":
                print(f"  â­ï¸ è·³è¿‡ combined ç›®å½•: {extra_path}")
                continue
            
            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
            if not (extra_path / "text").exists():
                print(f"  âš ï¸ è·³è¿‡ {extra_path.name}: ç¼ºå°‘ text æ–‡ä»¶")
                continue
            
            print(f"  ğŸ“‚ å¤„ç† {extra_path.name}...")
            extra_utt_count = 0
            
            for filename in files_to_merge:
                file_path = extra_path / filename
                # ä¹Ÿæ£€æŸ¥ .txt åç¼€ç‰ˆæœ¬
                if not file_path.exists():
                    file_path = extra_path / f"{filename}.txt"
                
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                merged_data[filename].append(line)
                                if filename == "text":
                                    extra_utt_count += 1
            
            # å¤„ç† spk2utt
            spk2utt_file = extra_path / "spk2utt"
            if spk2utt_file.exists():
                with open(spk2utt_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 2:
                            spk = parts[0]
                            utts = parts[1:]
                            if spk not in spk2utt_data:
                                spk2utt_data[spk] = []
                            spk2utt_data[spk].extend(utts)
            
            stats["total_utts"] += extra_utt_count
            stats["dialects_processed"] += 1
            print(f"     è¯­éŸ³æ•°: {extra_utt_count}")
    
    # é‡æ–°å†™å…¥åˆå¹¶æ–‡ä»¶ï¼ˆåŒ…å«é¢å¤–ç›®å½•çš„æ•°æ®ï¼‰
    print("\n  ğŸ“ å†™å…¥åˆå¹¶æ–‡ä»¶...")
    
    for filename, lines in merged_data.items():
        if lines:
            output_file = combined_dir / filename
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines) + '\n')
            print(f"     {filename}: {len(lines)} è¡Œ")
    
    # é‡æ–°å†™å…¥ spk2utt
    if spk2utt_data:
        spk2utt_file = combined_dir / "spk2utt"
        with open(spk2utt_file, 'w', encoding='utf-8') as f:
            for spk in sorted(spk2utt_data.keys()):
                utts = spk2utt_data[spk]
                f.write(f"{spk} {' '.join(utts)}\n")
        stats["total_speakers"] = len(spk2utt_data)
        print(f"     spk2utt: {len(spk2utt_data)} ä¸ªè¯´è¯äºº")
    
    print(f"\n  âœ… åˆå¹¶å®Œæˆ!")
    print(f"     æ–¹è¨€æ•°: {stats['dialects_processed']}")
    print(f"     è¯­éŸ³æ€»æ•°: {stats['total_utts']}")
    print(f"     è¯´è¯äººæ•°: {stats['total_speakers']}")
    
    return stats


def convert_audio_files(data_dir: Path, num_workers: int = 4) -> Tuple[int, int]:
    """å°† MP3 è½¬æ¢ä¸º WAV"""
    wav_scp = data_dir / "wav.scp"
    
    if not wav_scp.exists():
        print(f"  âŒ é”™è¯¯: {wav_scp} ä¸å­˜åœ¨")
        return 0, 0
    
    # è¯»å– wav.scp
    audio_files = []
    with open(wav_scp, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                utt_id = parts[0]
                src_path = parts[1]
                if src_path.endswith('.mp3'):
                    wav_path = src_path.replace('.mp3', '.wav')
                    audio_files.append((src_path, wav_path))
    
    if not audio_files:
        print("  âš ï¸ æ²¡æœ‰éœ€è¦è½¬æ¢çš„ MP3 æ–‡ä»¶")
        return 0, 0
    
    # å¹¶è¡Œè½¬æ¢
    success_count = 0
    fail_count = 0
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = {executor.submit(convert_single_audio, args): args for args in audio_files}
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="  è½¬æ¢è¿›åº¦"):
            success, result = future.result()
            if success:
                success_count += 1
            else:
                fail_count += 1
    
    print(f"  âœ… è½¬æ¢å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
    
    # æ›´æ–° wav.scp
    if success_count > 0:
        wav_scp_new = data_dir / "wav.scp.new"
        with open(wav_scp, 'r', encoding='utf-8') as f_in, \
             open(wav_scp_new, 'w', encoding='utf-8') as f_out:
            for line in f_in:
                parts = line.strip().split()
                if len(parts) >= 2:
                    utt_id = parts[0]
                    wav_path = parts[1].replace('.mp3', '.wav')
                    f_out.write(f"{utt_id} {wav_path}\n")
        
        # å¤‡ä»½åŸæ–‡ä»¶å¹¶æ›¿æ¢
        wav_scp_bak = data_dir / "wav.scp.bak"
        if not wav_scp_bak.exists():
            shutil.copy(wav_scp, wav_scp_bak)
        shutil.move(wav_scp_new, wav_scp)
        print(f"  âœ… æ›´æ–° wav.scp æŒ‡å‘ WAV æ–‡ä»¶")
    
    return success_count, fail_count


def validate_data(data_dir: Path, name: str = "") -> Dict[str, any]:
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    required_files = ["wav.scp", "text", "utt2spk", "spk2utt"]
    optional_files = ["instruct", "utt2embedding.pt", "spk2embedding.pt", "utt2speech_token.pt"]
    
    result = {"name": name, "valid": True}
    
    print(f"\n  ğŸ“ {name or data_dir.name}")
    print(f"  å¿…éœ€æ–‡ä»¶:")
    
    for f in required_files:
        exists = (data_dir / f).exists()
        result[f] = exists
        if not exists:
            result["valid"] = False
        status = "âœ…" if exists else "âŒ"
        
        # ç»Ÿè®¡è¡Œæ•°
        if exists:
            with open(data_dir / f, 'r', encoding='utf-8') as file:
                line_count = sum(1 for _ in file)
            print(f"    {status} {f} ({line_count} è¡Œ)")
        else:
            print(f"    {status} {f}")
    
    print(f"  å¯é€‰æ–‡ä»¶:")
    for f in optional_files:
        exists = (data_dir / f).exists()
        result[f] = exists
        status = "âœ…" if exists else "âšª"
        print(f"    {status} {f}")
    
    return result


def get_all_dialects(dataset_dir: Path) -> List[str]:
    """è·å–æ‰€æœ‰æ–¹è¨€ç›®å½•"""
    dialects = []
    if dataset_dir.exists():
        for item in dataset_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.') and item.name != COMBINED_DIR:
                # æ£€æŸ¥æ˜¯å¦åŒ…å« text æ–‡ä»¶
                if (item / "text").exists():
                    dialects.append(item.name)
    return sorted(dialects)


def main():
    parser = argparse.ArgumentParser(description="æ–¹è¨€è®­ç»ƒæ•°æ®å‡†å¤‡è„šæœ¬")
    parser.add_argument(
        "--mode",
        choices=["all", "instruct", "combine", "convert", "validate"],
        default="all",
        help="è¿è¡Œæ¨¡å¼: all=å®Œæ•´å‡†å¤‡, instruct=ç”Ÿæˆinstruct, combine=åˆå¹¶æ•°æ®, convert=è½¬æ¢éŸ³é¢‘, validate=éªŒè¯"
    )
    parser.add_argument(
        "--dataset-dir",
        default=DATASET_DIR,
        help=f"æ•°æ®é›†ç›®å½• (é»˜è®¤: {DATASET_DIR})"
    )
    parser.add_argument(
        "--dialects",
        nargs="*",
        default=None,
        help="æ–¹è¨€åˆ—è¡¨ï¼Œç•™ç©ºåˆ™è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰æ–¹è¨€"
    )
    parser.add_argument(
        "--combined-name",
        default=COMBINED_DIR,
        help=f"åˆå¹¶ç›®å½•å (é»˜è®¤: {COMBINED_DIR})"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="éŸ³é¢‘è½¬æ¢å¹¶è¡Œæ•° (é»˜è®¤: 4)"
    )
    parser.add_argument(
        "--no-convert",
        action="store_true",
        help="è·³è¿‡éŸ³é¢‘æ ¼å¼è½¬æ¢ï¼ˆç›´æ¥ä½¿ç”¨ MP3ï¼‰"
    )
    parser.add_argument(
        "--extra-dirs",
        nargs="*",
        default=[],
        help="é¢å¤–çš„æ•°æ®ç›®å½•ï¼ˆå¦‚ dataset_emotionï¼‰ï¼Œä¼šè¢«åˆå¹¶åˆ° combined ä¸­"
    )
    parser.add_argument(
        "--force-instruct",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆ instruct æ–‡ä»¶ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        default=None,
        help="åˆå¹¶è¾“å‡ºç›®å½•çš„ç»å¯¹è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º dataset-dir/combined-nameï¼‰"
    )
    
    args = parser.parse_args()
    
    script_dir = Path(__file__).parent
    dataset_dir = script_dir / args.dataset_dir
    
    # ç¡®å®šåˆå¹¶è¾“å‡ºç›®å½•
    if args.output_dir:
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ç»å¯¹è·¯å¾„
        combined_dir = Path(args.output_dir)
    else:
        # é»˜è®¤ä¸º dataset-dir/combined-name
        combined_dir = dataset_dir / args.combined_name
    
    # è‡ªåŠ¨æ£€æµ‹æ–¹è¨€
    if args.dialects is None or len(args.dialects) == 0:
        dialects = get_all_dialects(dataset_dir)
    else:
        dialects = args.dialects
    
    print("=" * 60)
    print("ğŸ—£ï¸  æ–¹è¨€è®­ç»ƒæ•°æ®å‡†å¤‡è„šæœ¬ v2.0")
    print("=" * 60)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"æ•°æ®é›†ç›®å½•: {dataset_dir}")
    print(f"æ£€æµ‹åˆ°æ–¹è¨€: {len(dialects)} ä¸ª")
    for d in dialects:
        name = DIALECT_NAMES.get(d, d)
        print(f"  - {d} ({name})")
    print(f"åˆå¹¶ç›®å½•: {combined_dir}")
    print("=" * 60)
    
    if not dialects:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–¹è¨€æ•°æ®ç›®å½•")
        sys.exit(1)
    
    # ==================== ç”Ÿæˆ instruct ====================
    if args.mode in ["all", "instruct"]:
        print("\n" + "=" * 40)
        print("ğŸ“ ç”Ÿæˆ instruct æ–‡ä»¶ï¼ˆä¸­æ–‡æŒ‡ä»¤ï¼‰")
        print("=" * 40)
        
        for dialect in dialects:
            data_dir = dataset_dir / dialect
            if data_dir.exists():
                print(f"\nå¤„ç† {dialect} ({DIALECT_NAMES.get(dialect, dialect)}):")
                generate_instruct_file(data_dir, dialect, force=args.force_instruct)
        
        # ä¹Ÿå¤„ç†é¢å¤–ç›®å½•çš„ instruct
        if args.extra_dirs:
            for extra_dir in args.extra_dirs:
                extra_path = script_dir / extra_dir
                if extra_path.exists():
                    print(f"\nå¤„ç†é¢å¤–ç›®å½• {extra_path.name}:")
                    generate_instruct_file(extra_path, extra_path.name, force=args.force_instruct)
    
    # ==================== éŸ³é¢‘è½¬æ¢ ====================
    if args.mode in ["all", "convert"] and not args.no_convert:
        print("\n" + "=" * 40)
        print("ğŸµ è½¬æ¢éŸ³é¢‘æ–‡ä»¶ (MP3 -> WAV)")
        print("=" * 40)
        
        if not check_ffmpeg():
            print("\nâš ï¸ æœªæ‰¾åˆ° ffmpegï¼Œè·³è¿‡éŸ³é¢‘è½¬æ¢")
            print("  å¦‚éœ€è½¬æ¢ï¼Œè¯·å®‰è£… ffmpeg")
        else:
            for dialect in dialects:
                data_dir = dataset_dir / dialect
                if data_dir.exists():
                    print(f"\nå¤„ç† {dialect}:")
                    convert_audio_files(data_dir, args.workers)
    
    # ==================== åˆå¹¶æ•°æ® ====================
    if args.mode in ["all", "combine"]:
        print("\n" + "=" * 40)
        print("ğŸ“¦ åˆå¹¶æ‰€æœ‰æ–¹è¨€æ•°æ®")
        print("=" * 40)
        
        # è§£æé¢å¤–ç›®å½•ä¸ºç»å¯¹è·¯å¾„
        extra_paths = []
        if args.extra_dirs:
            for extra_dir in args.extra_dirs:
                extra_path = script_dir / extra_dir
                if extra_path.exists():
                    extra_paths.append(extra_path)
                else:
                    print(f"  âš ï¸ é¢å¤–ç›®å½•ä¸å­˜åœ¨: {extra_dir}")
        
        combine_dialect_data(dataset_dir, dialects, combined_dir, extra_dirs=extra_paths)
    
    # ==================== éªŒè¯æ•°æ® ====================
    if args.mode in ["all", "validate"]:
        print("\n" + "=" * 40)
        print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§")
        print("=" * 40)
        
        # éªŒè¯å„æ–¹è¨€
        for dialect in dialects:
            data_dir = dataset_dir / dialect
            if data_dir.exists():
                validate_data(data_dir, DIALECT_NAMES.get(dialect, dialect))
        
        # éªŒè¯åˆå¹¶ç›®å½•
        if combined_dir.exists():
            print("\n" + "-" * 30)
            validate_data(combined_dir, "åˆå¹¶æ•°æ® (combined)")
    
    print("\n" + "=" * 60)
    print("âœ… å‡†å¤‡å®Œæˆ!")
    print("=" * 60)
    
    if args.mode == "all":
        print(f"""
ä¸‹ä¸€æ­¥æ“ä½œ:

1. è¿›å…¥è®­ç»ƒç›®å½•:
   cd CosyVoice/examples/dialect

2. ä¿®æ”¹ run.sh ä¸­çš„æ•°æ®ç›®å½•æŒ‡å‘åˆå¹¶æ•°æ®:
   data_dir=../../../dataset/{args.combined_name}

3. æŒ‰é˜¶æ®µæ‰§è¡Œè®­ç»ƒ:
   # Stage 1: æå– Speaker Embedding
   # Stage 2: æå– Speech Token
   # Stage 3: ç”Ÿæˆ Parquet
   # Stage 5: è®­ç»ƒæ¨¡å‹
   bash run.sh

åˆå¹¶æ•°æ®ä½ç½®: {combined_dir}
""")


if __name__ == "__main__":
    main()
